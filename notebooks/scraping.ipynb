{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import title_maker_pro.urban_dictionary_scraper\n",
    "import logging\n",
    "import pickle\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import stanza\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import OrderedDict\n",
    "from functools import partial\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from multiprocessing.pool import ThreadPool\n",
    "import io\n",
    "import itertools\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "session = urban_dictionary_scraper.get_session(throttle=0.1, expiry = (7*24*3600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.0.0.json: 120kB [00:00, 9.53MB/s]                    \n",
      "2023-01-06 23:23:31 INFO: Downloading default packages for language: en (English)...\n",
      "2023-01-06 23:23:31 INFO: File exists: /Users/jingchengyang/stanza_resources/en/default.zip.\n",
      "2023-01-06 23:23:34 INFO: Finished downloading models and saved to /Users/jingchengyang/stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "stanza.download('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "FeatureNotFound",
     "evalue": "Couldn't find a tree builder with the features you requested: html.parser. Do you need to install a parser library?",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFeatureNotFound\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[84], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m all_urls \u001B[38;5;241m=\u001B[39m \u001B[43murban_dictionary_scraper\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch_all_word_urls\u001B[49m\u001B[43m(\u001B[49m\u001B[43msession\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mall_urls.pickle\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwb\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m      3\u001B[0m     pickle\u001B[38;5;241m.\u001B[39mdump(all_urls, f, pickle\u001B[38;5;241m.\u001B[39mHIGHEST_PROTOCOL)\n",
      "File \u001B[0;32m~/Projects/AnglishGPT/title_maker_pro/urban_dictionary_scraper.py:164\u001B[0m, in \u001B[0;36mfetch_all_word_urls\u001B[0;34m(session, limit)\u001B[0m\n\u001B[1;32m    162\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, letter \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(letters):\n\u001B[1;32m    163\u001B[0m     logging\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mStarting fetch of words for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mletter\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 164\u001B[0m     all_definitions\u001B[38;5;241m.\u001B[39mupdate(\u001B[43mfetch_all_letter_word_url\u001B[49m\u001B[43m(\u001B[49m\u001B[43msession\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mletter\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    166\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m limit \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m i \u001B[38;5;241m>\u001B[39m limit:\n\u001B[1;32m    167\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m~/Projects/AnglishGPT/title_maker_pro/urban_dictionary_scraper.py:142\u001B[0m, in \u001B[0;36mfetch_all_letter_word_url\u001B[0;34m(session, letter, limit)\u001B[0m\n\u001B[1;32m    141\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfetch_all_letter_word_url\u001B[39m(session, letter, limit\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m--> 142\u001B[0m     first_ip \u001B[38;5;241m=\u001B[39m \u001B[43mfetch_letter_page\u001B[49m\u001B[43m(\u001B[49m\u001B[43msession\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mletter\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    144\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m first_ip\u001B[38;5;241m.\u001B[39mnum_pages:\n\u001B[1;32m    145\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFirst page of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mletter\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m lacks total number of pages!\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Projects/AnglishGPT/title_maker_pro/urban_dictionary_scraper.py:121\u001B[0m, in \u001B[0;36mfetch_letter_page\u001B[0;34m(session, letter, page)\u001B[0m\n\u001B[1;32m    119\u001B[0m     logging\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFetching \u001B[39m\u001B[38;5;132;01m{\u001B[39;00murl\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    120\u001B[0m character_page \u001B[38;5;241m=\u001B[39m get_with_retries(session, url)\n\u001B[0;32m--> 121\u001B[0m parsed_page \u001B[38;5;241m=\u001B[39m \u001B[43mBeautifulSoup\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcharacter_page\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mhtml.parser\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    122\u001B[0m last_string \u001B[38;5;241m=\u001B[39m parsed_page\u001B[38;5;241m.\u001B[39mbody\u001B[38;5;241m.\u001B[39mfind(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124ma\u001B[39m\u001B[38;5;124m\"\u001B[39m, string\u001B[38;5;241m=\u001B[39mre\u001B[38;5;241m.\u001B[39mcompile(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLast Â».*\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m    123\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m last_string:\n",
      "File \u001B[0;32m~/miniforge3/envs/anglish/lib/python3.10/site-packages/bs4/__init__.py:225\u001B[0m, in \u001B[0;36mBeautifulSoup.__init__\u001B[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001B[0m\n\u001B[1;32m    223\u001B[0m     builder_class \u001B[38;5;241m=\u001B[39m builder_registry\u001B[38;5;241m.\u001B[39mlookup(\u001B[38;5;241m*\u001B[39mfeatures)\n\u001B[1;32m    224\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m builder_class \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 225\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m FeatureNotFound(\n\u001B[1;32m    226\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCouldn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt find a tree builder with the features you \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    227\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrequested: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m. Do you need to install a parser library?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    228\u001B[0m             \u001B[38;5;241m%\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(features))\n\u001B[1;32m    230\u001B[0m \u001B[38;5;66;03m# At this point either we have a TreeBuilder instance in\u001B[39;00m\n\u001B[1;32m    231\u001B[0m \u001B[38;5;66;03m# builder, or we have a builder_class that we can instantiate\u001B[39;00m\n\u001B[1;32m    232\u001B[0m \u001B[38;5;66;03m# with the remaining **kwargs.\u001B[39;00m\n\u001B[1;32m    233\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m builder \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mFeatureNotFound\u001B[0m: Couldn't find a tree builder with the features you requested: html.parser. Do you need to install a parser library?"
     ]
    }
   ],
   "source": [
    "all_urls = urban_dictionary_scraper.fetch_all_word_urls(session)\n",
    "with open(\"all_urls.pickle\", \"wb\") as f:\n",
    "    pickle.dump(all_urls, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"all_urls.pickle\", \"rb\") as f:\n",
    "    to_fetch = pickle.load(f)\n",
    "    \n",
    "with open(\"all_words.pickle\", \"rb\") as f:\n",
    "    already_done = pickle.load(f)\n",
    "    for key in already_done.keys():\n",
    "        del to_fetch[key]\n",
    "        \n",
    "done = 100 * len(already_done) / (len(already_done) + len(to_fetch))\n",
    "print(f\"Done {done:.2f} percent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "t = ThreadPool(5)\n",
    "#with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "try:\n",
    "    fetch_all_definitions(session, to_fetch, already_done, save_interval=10000, executor=t)    \n",
    "finally:\n",
    "    t.terminate()\n",
    "    t.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open(\"data/all_words.pickle\", \"rb\") as f:\n",
    "    words = pickle.load(f)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def is_clean(word, min_upvotes=20, max_word_length=40, max_symbols=2, allow_upper=False, min_word_length=4):\n",
    "    if word.upvotes < min_upvotes:\n",
    "        return False\n",
    "    elif len(word.word) > max_word_length:\n",
    "        return False\n",
    "    elif len(word.word) < min_word_length:\n",
    "        return False\n",
    "    elif len(re.findall(r\"[^\\w .]\", word.word)) > max_symbols:\n",
    "        return False\n",
    "    elif not allow_upper and word.word.isupper():\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "clean_list = [\n",
    "    (k, title_maker_pro.urban_dictionary_scraper.UrbanDictionaryWord(\n",
    "        title=e.title,\n",
    "        url=e.url,\n",
    "        definitions=[x for x in e.definitions if is_clean(x)],\n",
    "    ))\n",
    "    for k,e in words.items() if any(is_clean(x) for x in e.definitions)\n",
    "]\n",
    "random.shuffle(clean_list)\n",
    "cleaned_words = OrderedDict(clean_list)\n",
    "\n",
    "print(f\"Words reduced by {len(cleaned_words) / len(words)}\")\n",
    "\n",
    "with open(\"data/cleaned_words_all_def_min_upvotes_20_max_len_40_min_len_4_no_upper_randomized.pickle\", \"wb\") as f:\n",
    "    pickle.dump(cleaned_words, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nlp = stanza.Pipeline(processors=\"tokenize,pos\")\n",
    "def proper_noun_guess(word):\n",
    "    query = word.title.upper().strip().strip(\"\\\"\").strip()\n",
    "    for definition in word.definitions:\n",
    "        try:\n",
    "            doc = nlp(definition.examples[0])\n",
    "        except IndexError:\n",
    "            print(f\"{query}: INDEX ERROR\")\n",
    "            return False\n",
    "        for sentence in doc.sentences:\n",
    "            last_prop = []\n",
    "            for word in sentence.words:\n",
    "                if word.upos == \"PROPN\":\n",
    "                    last_prop.append(word.text.upper())\n",
    "                    if query == \" \".join(last_prop):\n",
    "                        return True\n",
    "                else:\n",
    "                    last_prop = []\n",
    "               \n",
    "pbar = tqdm(total=len(cleaned_words.values()))\n",
    "for i, item in enumerate(cleaned_words.values()):\n",
    "    t = proper_noun_guess(item)\n",
    "    if t:\n",
    "        print(f\"{item.title}: {t}\")\n",
    "        \n",
    "    pbar.update()\n",
    "    \n",
    "    if i > 1000:\n",
    "        break\n",
    "    \n",
    "proper_noun_guess(next(iter(words.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "defns = pd.DataFrame(\n",
    "    [\n",
    "        [e.word, e.meaning, e.examples[0], e.creation_epoch, e.upvotes, e.downvotes]\n",
    "        for e in itertools.chain.from_iterable(e.definitions for e in words.values())\n",
    "    ],\n",
    "    columns=[\"word\", \"meaning\", \"example\", \"creation_epoch\", \"upvotes\", \"downvotes\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "smoothing_prior = 20\n",
    "defns[\"smoothed_upvotes\"] = defns[\"upvotes\"] / (defns[\"upvotes\"] + defns[\"downvotes\"] + smoothing_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "defns[\"smoothed_upvotes\"].quantile(np.linspace(0.1, 1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cleaned_defs = defns[:]\n",
    "# cleaned_defs = cleaned_defs[cleaned_defs[\"smoothed_upvotes\"] >= 0.2]\n",
    "cleaned_defs = cleaned_defs[cleaned_defs[\"upvotes\"] >= 20]\n",
    "cleaned_defs = cleaned_defs[cleaned_defs.word.str.len() <= 40]\n",
    "cleaned_defs = cleaned_defs[cleaned_defs.word.str.len() >= 4]\n",
    "cleaned_defs = cleaned_defs[~cleaned_defs.word.str.isupper()]\n",
    "\n",
    "cleaned_defs = cleaned_defs[cleaned_defs.word.str.count(\"[^\\w .]\") <= 2]\n",
    "print(f\"Reduction from {len(defns)} to {len(cleaned_defs)} ({len(cleaned_defs) / len(defns)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cleaned_defs[cleaned_defs.word.str.upper().str.contains(\",\")].sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "defns.word.str.count(\"[^\\w ].\").describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "defns[defns.word.str.len() > 40].sample(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "defns[defns.word.str.count(\"[^\\w .]\") > 2].sample(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "(defns[\"meaning\"].str.len() + defns[\"example\"].str.len()).quantile(np.linspace(0.01, 1, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lng_defs = defns[defns[\"meaning\"].str.len() > 985]\n",
    "(lng_defs[\"upvotes\"] + lng_defs[\"downvotes\"]).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lng_defs = defns[defns[\"meaning\"].str.len() < 985]\n",
    "(lng_defs[\"upvotes\"] + lng_defs[\"downvotes\"]).describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}